from __future__ import annotations

import os
from typing import Dict, Any

import pandas as pd
from gpt4all import GPT4All


class LocalPipeline:
    def __init__(self, model_name: str | None = None, model_path: str = 'models/', *, filter_model: str | None = None, abstract_model: str | None = None, summarize_model: str | None = None, overall_model: str | None = None, impact_model: str | None = None, finetuned_trigger_name: str | None = None) -> None:
        abs_model_path = os.path.abspath(model_path)
        # å•æ¨¡å‹æˆ–åˆ†é˜¶æ®µæ¨¡å‹è£…é…
        self.model_path = abs_model_path
        self.model_single = None
        if model_name:
            self.model_single = GPT4All(model_name, model_path=abs_model_path, allow_download=False)
        self.model_filter_name = filter_model
        self.model_abstract_name = abstract_model
        self.model_summarize_name = summarize_model
        # æ–°å¢ï¼šç”¨äºâ€œå¤šæ®µJSONæ±‡æ€»ä¸º1ä¸ªæœ€ä½³JSONâ€çš„ä¸“ç”¨æ¨¡å‹
        self.model_overall_name = overall_model
        # æ–°å¢ï¼šImpact é˜¶æ®µä¸“ç”¨æ¨¡å‹ï¼ˆä¾‹å¦‚åŸå§‹ meta-llama-3.1-8bï¼‰
        self.model_impact_name = impact_model
        self.finetuned_trigger_name = finetuned_trigger_name or 'My_Finetuned_Model'
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ¨¡å‹åŠ è½½æƒ…å†µ
        if os.getenv("FCPD_DEBUG", "0") == "1":
            print(f"  [Debug] LocalPipeline Init: Summarize='{self.model_summarize_name}', Overall='{self.model_overall_name}'")
            print(f"  [Debug] Finetuned Trigger: '{self.finetuned_trigger_name}'")

    def _create_prompt(self, system: str, user: str, context: str = "") -> str:
        """åˆ›å»ºæ™®é€šæ ¼å¼çš„ prompt"""
        parts = []
        if system:
            parts.append(system)
        if context:
            parts.append(f"Context\n{context}")
        parts.append(f"Task\n{user}")
        return "\n\n".join(parts)
    def _create_llama31_chat_prompt(self, system: str, user: str, context: str = "") -> str:
        """åˆ›å»º Llama 3.1 chat template æ ¼å¼çš„ promptï¼ˆç”¨äºå¾®è°ƒæ¨¡å‹ï¼‰"""
        user_content = ""
        if context:
            user_content += f"Context\n{context}\n\n"
        user_content += f"Task\n{user}"
        
        # Llama 3.1 chat template æ ¼å¼
        prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{user_content}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
        return prompt

    def _is_finetuned_model(self, model_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å¾®è°ƒæ¨¡å‹"""
        # 1. ç¯å¢ƒå˜é‡å¼ºåˆ¶è¦†ç›–
        if os.getenv('FCPD_FORCE_FINETUNED') == '1':
            return True
        if not model_name:
            return False
        # 2. åç§°åŒ¹é…
        is_ft = self.finetuned_trigger_name.lower() in model_name.lower()
        # if os.getenv("FCPD_DEBUG", "0") == "1":
        #     print(f"  [Debug] Is Finetuned? Model='{model_name}' vs Trigger='{self.finetuned_trigger_name}' -> {is_ft}")
        return is_ft

    def _get_chat_prompt(self, system: str, user: str, context: str = "", stage: str = "filter") -> str:
        """æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹© prompt æ ¼å¼"""
        # åˆ¤æ–­å½“å‰é˜¶æ®µçš„æ¨¡å‹æ˜¯å¦æ˜¯å¾®è°ƒæ¨¡å‹
        if stage == 'summarize' and self.model_summarize_name:
            if self._is_finetuned_model(self.model_summarize_name):
                return self._create_llama31_chat_prompt(system, user, context)
        if stage == 'overall' and self.model_overall_name:
            if self._is_finetuned_model(self.model_overall_name):
                return self._create_llama31_chat_prompt(system, user, context)
        return self._create_prompt(system, user, context)

    

    def _clip(self, text: str, max_chars: int) -> str:
        if not text:
            return text
        if len(text) <= max_chars:
            return text
        return text[:max_chars]

    def _safe_generate(self, model: GPT4All, prompt: str, *, max_tokens: int, temp: float = 0.0) -> str:
        try:
            out = (model.generate(prompt=prompt, max_tokens=max_tokens, temp=temp) or '').strip()
            # è‹¥æ¨¡å‹è¿”å›ä¸Šä¸‹æ–‡è¶…é™ç±»é”™è¯¯æ–‡æœ¬ï¼Œè€ŒéæŠ›å¼‚å¸¸ï¼Œåˆ™ä¸»åŠ¨é™è½½é‡è¯•
            low = out.lower()
            if ("context window" in low and "exceed" in low) or ("prompt size" in low and "context window" in low):
                clipped = self._clip(prompt, 2000)
                out2 = (model.generate(prompt=clipped, max_tokens=max_tokens - 100, temp=temp) or '').strip()
                low2 = out2.lower()
                if ("context window" in low2 and "exceed" in low2) or ("prompt size" in low2 and "context window" in low2):
                    clipped = self._clip(prompt, 1500)
                    return (model.generate(prompt=clipped, max_tokens=max_tokens - 200, temp=temp) or '').strip()
                return out2
            return out
        except Exception:
            # äºŒæ¬¡é™è½½ï¼šå‡å°‘promptä¸max_tokensåé‡è¯•
            clipped = self._clip(prompt, 2000)
            try:
                return (model.generate(prompt=clipped, max_tokens=max_tokens - 100, temp=temp) or '').strip()
            except Exception:
                # æœ€ç»ˆå…œåº•
                clipped = self._clip(prompt, 1500)
                return (model.generate(prompt=clipped, max_tokens=max_tokens - 200, temp=temp) or '').strip()

    def _pack_adjacent_rows(self, df: pd.DataFrame, column: str, *, min_chars: int, max_chars: int) -> pd.DataFrame:
        """å°†ç›¸é‚»è¡ŒæŒ‰é¡ºåºåˆå¹¶åˆ°[min_chars, max_chars]èŒƒå›´ï¼Œé¿å…çŸ­å¥ä¿¡æ¯ä¸è¶³ï¼Œæ§åˆ¶æ€»é•¿é¿å…æº¢å‡ºã€‚"""
        if column not in df.columns:
            return df
        packed_rows: list[str] = []
        cur_parts: list[str] = []
        cur_len = 0
        for _, row in df.iterrows():
            s = str(row.get(column, "") or "")
            if not s.strip():
                continue
            if cur_len == 0:
                # èµ·å§‹ç›´æ¥åŠ å…¥
                cur_parts = [s]
                cur_len = len(s)
                # å•æ®µè¶…é•¿ï¼šç›´æ¥è¾“å‡ºä¸ºä¸€ä¸ªå—ï¼ˆä¸åˆ‡åˆ†ï¼‰
                if cur_len >= max_chars:
                    packed_rows.append(s[:max_chars])
                    cur_parts, cur_len = [], 0
                continue
            # å°è¯•åˆå¹¶
            candidate_len = cur_len + 1 + len(s)
            if candidate_len <= max_chars:
                cur_parts.append(s)
                cur_len = candidate_len
                continue
            # è¾¾åˆ°ä¸Šé™ï¼šè‹¥å½“å‰å—ä¸è¶³ä¸‹é™ï¼Œä»ç„¶å…ˆè¾“å‡ºä»¥ä¿è¯é¡ºåºï¼›å¦åˆ™è¾“å‡ºå½“å‰å—
            packed_rows.append(" ".join(cur_parts))
            # å¼€å¯æ–°çš„å—
            cur_parts = [s]
            cur_len = len(s)
            if cur_len >= max_chars:
                packed_rows.append(s[:max_chars])
                cur_parts, cur_len = [], 0
        if cur_len > 0 and cur_parts:
            # æœ«å°¾å—
            if cur_len < min_chars and packed_rows:
                # è‹¥æœ«å°¾ä¸è¶³ä¸‹é™ï¼Œå°½é‡ä¸ä¸Šä¸€å—åˆå¹¶ï¼ˆä¸è¶…è¿‡ä¸Šé™ï¼‰
                prev = packed_rows.pop()
                merged = (prev + " " + " ".join(cur_parts)).strip()
                if len(merged) <= max_chars:
                    packed_rows.append(merged)
                else:
                    packed_rows.append(prev)
                    packed_rows.append(" ".join(cur_parts))
            else:
                packed_rows.append(" ".join(cur_parts))
        return pd.DataFrame(packed_rows, columns=[column])

    def save_df_to_text(self, df: pd.DataFrame, file_path: str, content_column: str = 'content') -> None:
        with open(file_path, 'w', encoding='utf-8') as f:
            for _, row in df.iterrows():
                f.write(str(row.get(content_column, "")) + "\n\n")

    def _smart_pack_rows(self, df: pd.DataFrame, column: str, *, max_chars: int = 1500, window: int = 1) -> pd.DataFrame:
        """
        æ™ºèƒ½ç›¸é‚»æ‰“åŒ…ï¼šå½“æŸè¡ŒåŒ…å«é«˜æ”¶ç›Š/é«˜è½¬åŒ–ä¿¡å·ï¼Œä½†ç¼ºå°‘æ¡ä»¶å•ä½æ—¶ï¼Œä¸ç›¸é‚»Â±windowè¡Œæ‹¼æ¥ï¼Œé•¿åº¦ä¸è¶…è¿‡ max_charsã€‚
        è§¦å‘å¯å‘å¼ï¼š
          - æ–‡æœ¬å«ç™¾åˆ†å·æˆ–å…³é”®è¯ (yield|conversion|selectivity)
          - ä¸”ä¸å«æ¸©åº¦/æ—¶é—´/å‹åŠ›/æ€»æµé‡å•ä½ï¼ˆå…è®¸åç»­ Summarize æ›´å®¹æ˜“å°±è¿‘æ•è·æ¡ä»¶ï¼‰
        """
        if column not in df.columns:
            return df
        import re
        texts = [str(x or "") for x in df[column].tolist()]
        n = len(texts)
        if n == 0:
            return df
        percent_re = re.compile(r"\b\d{1,3}\s?%\b")
        outcomes_re = re.compile(r"\b(yield|conversion|selectivity)\b", re.IGNORECASE)
        # æ¡ä»¶å•ä½ï¼šæ¸©åº¦/æ—¶é—´/å‹åŠ›/æ€»æµé‡
        units_re = re.compile(
            r"(?:\b(?:Â°C|K|Â°F)\b|\b(?:\d+\.?\d*)\s*(?:min|s|h)\b|\b(?:bar|MPa)\b|\bmL\s*/\s*(?:min|h)\b|\bÂµL\s*/\s*min\b|\buL\s*/\s*min\b)",
            re.IGNORECASE,
        )
        used = [False] * n
        packed: list[str] = []
        for i in range(n):
            if used[i]:
                continue
            base = texts[i]
            t_low = base.lower()
            has_result = bool(percent_re.search(base) or outcomes_re.search(t_low))
            lacks_units = not bool(units_re.search(base))
            if has_result and lacks_units:
                # è§¦å‘æ™ºèƒ½æ‹¼æ¥ï¼šä¸ç›¸é‚»Â±windowæ‹¼æ¥ï¼ˆä¼˜å…ˆåï¼Œå†å‰ï¼‰ï¼Œé™åˆ¶é•¿åº¦
                combined = base
                used[i] = True
                # å‘å
                for off in range(1, window + 1):
                    j = i + off
                    if j < n and not used[j] and len(combined) + 1 + len(texts[j]) <= max_chars:
                        combined = combined + " " + texts[j]
                        used[j] = True
                # å‘å‰
                for off in range(1, window + 1):
                    j = i - off
                    if j >= 0 and not used[j] and len(texts[j]) + 1 + len(combined) <= max_chars:
                        combined = texts[j] + " " + combined
                        used[j] = True
                packed.append(combined)
            else:
                used[i] = True
                packed.append(base)
        return pd.DataFrame(packed, columns=[column])

    def filter_content(self, df: pd.DataFrame) -> pd.DataFrame:
        user_question = (
            "Does this paragraph describe the authors' OWN experimental work, results, or conclusions in THIS SPECIFIC study? "
            "Answer 'Yes' ONLY if it details new experiments, specific conditions (temp/time/yield), or results performed by the authors.\n"
            "Answer 'No' if it is Introduction, Background, Literature Review (citing previous work/references), general theory, or future plans.\n"
            "Strictly answer 'Yes' or 'No'."
        )
        system_prompt = (
            "You are an expert assistant for scientific literature mining. "
            "Keep experimental sections, results, abstracts, and conclusions. Discard background/intro."
        )
        results = []
        for _, row in df.iterrows():
            content = self._clip(str(row['content']), 1200)
            # æ‰©å±•å…³é”®è¯ï¼šå¢åŠ ç« èŠ‚æ ‡é¢˜ç±»å¼ºä¿¡å·ï¼Œç›´æ¥ä¿ç•™
            kw_strong = [
                "abstract", "conclusion", "experimental", "experimental section",
                "materials and methods", "methods", "procedure", "general procedure",
                "typical procedure", "optimization", "best conditions", "results and discussion"
            ]
            # kw_technical å·²è¢«ç§»é™¤ï¼Œä¸å†ç”¨äºè·³è¿‡ LLM æ£€æŸ¥
            content_lower = content.lower()
            # 1) æ ‡é¢˜çª—å£å‘½ä¸­ï¼šå‰ N å­—ç¬¦å†…åŒ…å«å¼ºä¿¡å·æ ‡é¢˜åˆ™ç›´æ¥ä¿ç•™
            try:
                title_window = int(os.getenv("FCPD_FILTER_TITLE_WINDOW", "80"))
            except Exception:
                title_window = 80
            prefix = content_lower[:max(0, title_window)]
            if any(k in prefix for k in kw_strong):
                results.append('Yes')
                continue
            # 2) æŠ€æœ¯å…³é”®è¯å‘½ä¸­ -> å¼ºåˆ¶äº¤ç»™ LLM åˆ¤æ–­ï¼Œä»¥å‡å°‘è¯¯æŠ¥
            # æ¢å¤éƒ¨åˆ†æé«˜ç½®ä¿¡åº¦çš„æŠ€æœ¯æŒ‡æ ‡ç›´é€šï¼Œä»¥é¿å… LLM è¿‡åº¦è¿‡æ»¤
            # è§„åˆ™ï¼šå¿…é¡»åŒ…å«æ˜ç¡®çš„æ•°å€¼ç»“æœï¼ˆ%ï¼‰æˆ–å…³é”®æµç¨‹å‚æ•°ï¼ˆresidence timeï¼‰
            if "residence time" in content_lower:
                results.append('Yes')
                continue
            if "%" in content and ("yield" in content_lower or "conversion" in content_lower):
                # ç®€å•çš„æ­£åˆ™æ£€æŸ¥æ•°å€¼
                import re
                if re.search(r"\d+\s*%", content):
                    results.append('Yes')
                    continue
            
            # 3) å…¶å®ƒæƒ…å†µäº¤ç»™ LLM åˆ¤æ–­
            prompt = self._create_prompt(system_prompt, user_question, content)
            model = self._get_stage_model('filter')
            resp = self._safe_generate(model, prompt, max_tokens=5, temp=0.0)
            results.append('Yes' if resp.strip().lower().startswith('yes') else 'No')
        out = df.copy()
        out['classification'] = results
        return out[out['classification'] == 'Yes'].copy()

    def abstract_text(self, df: pd.DataFrame) -> pd.DataFrame:
        user_prompt = (
            "Summarize the paragraph focusing on flow-chemistry process development. "
            "Highlight: reaction type, key reactants/solvent/catalyst, products, reactor details (type/ID), "
            "critical conditions (flow rate(s), residence time, temperature, pressure), and outcomes (conversion/yield/selectivity). "
            "Be concise, faithful to text, no speculation."
        )
        system_prompt = (
            "You are an expert assistant for scientific literature mining. Return concise, faithful summaries."
        )
        abstracts = []
        for _, row in df.iterrows():
            content = self._clip(str(row['content']), 2000)
            prompt = self._create_prompt(system_prompt, user_prompt, content)
            model = self._get_stage_model('abstract')
            text = self._safe_generate(model, prompt, max_tokens=280, temp=0.0)
            abstracts.append(text.strip() or content[:400])
        out = df.copy()
        out['abstract'] = abstracts
        return out

    def _sanitize_json_text(self, text: str) -> str:
        import re
        s = text or ""
        # 1) å»é™¤ markdown å›´æ 
        s = re.sub(r"```json\s*", "", s, flags=re.IGNORECASE)
        s = re.sub(r"```\s*", "", s)
        # 2) ä»…ä¿ç•™æœ€å¤§å®Œæ•´ { ... } å—
        brace_count = 0
        start = -1
        candidates = []
        for i, ch in enumerate(s):
            if ch == '{':
                if brace_count == 0:
                    start = i
                brace_count += 1
            elif ch == '}':
                if brace_count > 0:
                    brace_count -= 1
                    if brace_count == 0 and start != -1:
                        candidates.append(s[start:i+1])
                        start = -1
        if candidates:
            s = max(candidates, key=len)
        # 3) å»æ³¨é‡Šä¸å°¾é€—å·
        s = re.sub(r"//.*?(?=\n|$)", "", s)
        s = re.sub(r"/\*[\s\S]*?\*/", "", s)
        s = re.sub(r",\s*(\}|\])", r"\1", s)
        # 4) æ•°å€¼åŒ–ï¼ˆå°† "82" â†’ 82ï¼Œä»…é™çº¯æ•°å­—çš„å€¼ï¼‰
        s = re.sub(r':\s*"(-?\d+\.?\d*)"\s*([,\}])', r': \1\2', s)
        # 5) å°†æœªåŠ å¼•å·çš„çœç•¥å·ï¼ˆ... æˆ– â€¦ï¼‰æ ‡å‡†åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… JSON è§£æå¤±è´¥
        s = re.sub(r':\s*(\.\.\.|â€¦)\s*([,\}\]])', r': "..." \2', s)
        return s.strip()

    def _keep_best_only(self, json_text: str) -> str:
        import json
        try:
            obj = json.loads(json_text)
        except Exception:
            return json_text
        
        # 1. ç»“æ„æ ‡å‡†åŒ–ï¼šå°† reaction_summary å†…åµŒçš„ products/metrics/reactants/conditions ç§»åˆ°é¡¶å±‚
        rs = obj.get("reaction_summary", {}) or {}
        if isinstance(rs, dict):
            if "products" in rs and isinstance(rs["products"], list):
                obj.setdefault("products", []).extend(rs["products"])
                rs.pop("products", None)
            if "metrics" in rs and isinstance(rs["metrics"], dict):
                if not obj.get("metrics"):
                    obj["metrics"] = rs["metrics"]
                rs.pop("metrics", None)
            if "reactants" in rs and isinstance(rs["reactants"], list):
                obj.setdefault("reactants", []).extend(rs["reactants"])
                rs.pop("reactants", None)
            if "conditions" in rs and isinstance(rs["conditions"], list):
                obj.setdefault("conditions", []).extend(rs["conditions"])
                rs.pop("conditions", None)

        # 2. ç¡®ä¿åˆ—è¡¨å­—æ®µç±»å‹æ­£ç¡®
        for k in ["products", "reactants", "conditions"]:
            if not isinstance(obj.get(k), list):
                obj[k] = []
        
        try:
            return json.dumps(obj, ensure_ascii=False)
        except Exception:
            return json_text
            # return json_text
    def _normalize_reaction_type(self, value: str | None) -> str | None:
        if not value or not isinstance(value, str):
            return None
        v = value.strip().lower()
        if not v:
            return None
        synonyms = {
            "hydrogenation": {"hydrogenation", "h2 addition"},
            "nitration": {"nitration"},
            "oxidation": {"oxidation", "oxidative"},
            "reduction": {"reduction", "reductive"},
            "photoreduction": {"photoreduction", "photo-reduction"},
            "esterification": {"esterification"},
            "amidation": {"amidation"},
            "halogenation": {"halogenation", "chlorination", "bromination", "fluorination", "iodination"},
            "alkylation": {"alkylation"},
            "acylation": {"acylation", "friedel-crafts acylation"},
            "isomerization": {"isomerization"},
            "dehydration": {"dehydration"},
            "dehydrogenation": {"dehydrogenation"},
            "polymerization": {"polymerization"},
            "coupling": {"coupling", "suzuki coupling", "heck coupling", "sonogashira coupling"},
            "hydrolysis": {"hydrolysis"},
            "photocatalysis": {"photocatalysis", "photocatalytic"},
        }
        non_types = {
            "conversion", "selectivity", "yield", "production", "regime", "microfluidic regime",
            "batch synthesis", "gas-liquid system", "gas-liquid catalytic reaction", "bayesian optimization",
            "synthesis", "process", "purge"
        }
        if v in non_types:
            return None
        for norm, alts in synonyms.items():
            if v in alts:
                return norm
        for norm, alts in synonyms.items():
            for a in alts:
                if a in v:
                    return norm
        strict = os.getenv("FCPD_RXN_TAXONOMY_STRICT", "1") == "1"
        return None if strict else v
    def _clean_chem_name(self, name: str | None) -> str | None:
        if not name or not isinstance(name, str):
            return None
        n = name.strip()
        if not n: return None
        # 1. åŸºç¡€æ¸…æ´—
        # å»é™¤é¦–å°¾æ ‡ç‚¹
        n = n.strip(".,;:")
        # 2. å¸¸è§æ ¼å¼é”™è¯¯ä¿®æ­£ (å¯æŒ‰éœ€æ‰©å±•)
        # e.g. "4-chloro nitrobenzene" -> "4-chloronitrobenzene" (ä»…ä½œç¤ºä¾‹ï¼Œæ­¤å¤„é€»è¾‘è¾ƒæ¿€è¿›ï¼Œéœ€è°¨æ…)
        # ä¿®æ­£å­—å…¸ (ç”¨æˆ·å¯åœ¨æ­¤æ·»åŠ ç‰¹å®šæ˜ å°„)
        corrections = {
            "nltrobenzene": "nitrobenzene",
            # "nitrosobenzene": "nitrobenzene", # è­¦å‘Šï¼šè¿™æ˜¯ä¸åŒç‰©è´¨ï¼Œä»…å½“ç¡®ä¿¡æ˜¯è¯¯è¯†æ—¶æ‰å¯ç”¨
        }
        if n.lower() in corrections:
            return corrections[n.lower()]
        
        # 3. æ‹¬å·ç¼–å·å»é™¤ï¼šå¦‚ "Compound (1)" -> "Compound"
        import re
        # å»é™¤æœ«å°¾çš„ "(æ•°å­—/å­—æ¯)" å¼•ç”¨
        n = re.sub(r'\s*\(\s*\d+[a-z]?\s*\)$', '', n)
        # å»é™¤æœ«å°¾çš„çº¯æ•°å­—ç¼–å· "Product 3" -> "Product"
        n = re.sub(r'\s+\d+$', '', n)
        
        return n if len(n) > 1 else None

    def _ensure_schema_and_order(self, obj_in: dict) -> dict:
        import copy
        obj = copy.deepcopy(obj_in) if isinstance(obj_in, dict) else {}
        rs = obj.get("reaction_summary") or {}
        if not isinstance(rs, dict):
            rs = {}
        rxn_type = rs.get("reaction_type") or obj.get("reaction_type")
        rxn_type = self._normalize_reaction_type(rxn_type if isinstance(rxn_type, str) else None)
        reactants = obj.get("reactants")
        if not isinstance(reactants, list):
            reactants = rs.get("reactants") if isinstance(rs.get("reactants"), list) else []
        products = obj.get("products")
        if not isinstance(products, list):
            products = rs.get("products") if isinstance(rs.get("products"), list) else []
        
        # è‡ªåŠ¨ä¿®æ­£ï¼šæ ¹æ® role å­—æ®µè°ƒæ•´å½’å±
        final_reactants = []
        final_products = list(products) # å¤åˆ¶ä¸€ä»½
        
        for r in reactants:
            if not isinstance(r, dict): 
                continue
            # æ¸…æ´—åç§°
            r["name"] = self._clean_chem_name(r.get("name"))
            if not r["name"]: continue
            
            role = str(r.get("role", "")).lower()
            name = r.get("name")
            if "product" in role and "reactant" not in role:
                # è¯¯å…¥ reactants çš„ productï¼Œç§»åŠ¨è¿‡å»
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if not any(p.get("name") == name for p in final_products):
                    final_products.append({"name": name, "yield_optimal": None, "unit": "%"})
            else:
                final_reactants.append(r)
        
        # æ¸…æ´— products åç§°
        for p in final_products:
            if isinstance(p, dict):
                p["name"] = self._clean_chem_name(p.get("name"))
        # è¿‡æ»¤æ‰æ¸…æ´—ååå­—ä¸ºç©ºçš„
        final_products = [p for p in final_products if p.get("name") or p.get("yield_optimal")]

        reactants = final_reactants
        products = final_products

        conditions = obj.get("conditions")
        if not isinstance(conditions, list):
            conditions = rs.get("conditions") if isinstance(rs.get("conditions"), list) else []
        reactor = obj.get("reactor")
        if not isinstance(reactor, dict):
            reactor = rs.get("reactor") if isinstance(rs.get("reactor"), dict) else {}
        metrics = obj.get("metrics")
        if not isinstance(metrics, dict):
            metrics = rs.get("metrics") if isinstance(rs.get("metrics"), dict) else {}
        target_types = ["temperature", "residence_time", "flow_rate_total", "pressure"]
        def _find_cond_value(t, conds):
            for c in conds if isinstance(conds, list) else []:
                if isinstance(c, dict) and c.get("type") == t:
                    return c.get("value")
            return None
        fixed_conds = []
        for t in target_types:
            v = _find_cond_value(t, conditions)
            v = v if isinstance(v, str) and v.strip() else None
            fixed_conds.append({"type": t, "value": v})
        conv = metrics.get("conversion") if isinstance(metrics, dict) else None
        yld = metrics.get("yield") if isinstance(metrics, dict) else None
        sel = metrics.get("selectivity") if isinstance(metrics, dict) else None
        fixed_metrics = {
            "conversion": conv if isinstance(conv, (int, float)) else None,
            "yield": yld if isinstance(yld, (int, float)) else None,
            "selectivity": sel if isinstance(sel, (int, float)) else None,
            "unit": "%"
        }
        fixed_reactor = {
            "type": reactor.get("type") if isinstance(reactor, dict) else None,
            "inner_diameter": reactor.get("inner_diameter") if isinstance(reactor, dict) else None
        }
        out = {}
        out["reaction_summary"] = {"reaction_type": rxn_type}
        out["reactants"] = [r for r in reactants if isinstance(r, dict)]
        out["products"] = [p for p in products if isinstance(p, dict)]
        out["conditions"] = fixed_conds
        out["reactor"] = fixed_reactor
        out["metrics"] = fixed_metrics
        return out

    def summarize_parameters(self, df: pd.DataFrame) -> pd.DataFrame:
        # è·¯å¾„åˆ†æ”¯ï¼šå¾®è°ƒæ¨¡å‹ä½¿ç”¨ç²¾ç®€ Promptï¼›æœªå¾®è°ƒä¿æŒåŸæœ‰ä¸¥æ ¼ç­–ç•¥
        use_finetuned = bool(self.model_summarize_name) and self._is_finetuned_model(self.model_summarize_name)
        if use_finetuned:
            system_prompt = "You output ONLY valid JSON. No explanations."
            user_prompt = (
                "Only use THIS paragraph; do not infer across other paragraphs.\n"
                "NEGATIVE CONSTRAINTS: If the paragraph describes 'previous work', 'literature review', or 'background theory', ONLY extract data if it is explicitly compared with or part of the current study's experimental results. Otherwise, return a JSON with all nulls. Do NOT extract data cited from other papers unless relevant to the current study's optimization.\n\n"
                "SPECIAL ATTENTION (Gases & Solvents):\n"
                "- Explicitly extract gaseous reactants (e.g., Hydrogen/H2, Oxygen/O2, CO) if flow rates or partial pressures are mentioned, even if not in a 'reagents' list.\n"
                "- Include simple molecules (e.g., ethylene, benzene, toluene) as reactants/products if they are the main reaction components.\n\n"
                # "For conditions, prefer values with units (Â°C/K/Â°F; s/min/h; bar/MPa; mL/min). Ignore counts like '7 experiments'.\n"
                # "If a result (yield/conversion) is mentioned, associate the nearest temperature/time in the same or preceding sentence.\n\n"
                "Schema (keys in this exact order):\n"
                "1) reaction_summary.reaction_type\n"
                "2) reactants[{name,role}] (include gases like H2/O2 if used)\n"
                "3) products[{name,yield_optimal,unit}]\n"
                "4) conditions[{type,value}]  (types: temperature,residence_time,flow_rate_total,pressure)\n"
                "5) reactor{type,inner_diameter}\n"
                "6) metrics{conversion,yield,selectivity,unit}\n\n"
                "Reaction type taxonomy (choose ONE label, otherwise null): hydrogenation, nitration, oxidation, reduction, photoreduction, esterification, amidation, halogenation, alkylation, acylation, isomerization, dehydration, dehydrogenation, polymerization, coupling, hydrolysis, photocatalysis.\n"
                "Do NOT output non-types such as conversion, batch synthesis, microfluidic regime, gas-liquid system, Bayesian optimization, synthesis, process, purge.\n\n"
                "Rules:\n"
                "- Extract values exactly as they appear; if not present, use null.\n"
                "- Prefer full chemical names with abbreviations (e.g. \"3-methyl-2-nitrobenzoic acid (MNA)\") if available in text.\n"
                "- No guessing or inventions; use null/empty arrays when unknown.\n"
                "- Output ONLY valid JSON without extra text.\n"
            )
        else:
            # 3. å›é€€åˆ°ç±»ä¼¼äº Baseline (Summerized.py) çš„å¯å‘å¼æå–ç­–ç•¥
            # å…è®¸æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­æœ€ä½³æ¡ä»¶ï¼Œè€Œä¸æ˜¯æ­»æ¿åœ°ä»…æå–æ˜¾å¼å£°æ˜
            system_prompt = (
                "You are an expert assistant for scientific literature mining. "
                "Your goal is to extract the SINGLE BEST reaction condition set from the text."
            )
            user_prompt = (
                "Analyze the provided paragraph regarding flow-chemistry process development.\n"
                "Task:\n"
                "1. Identify the MAIN reaction described (Authors' OWN work only).\n"
                "2. Extract the OPTIMAL/BEST condition set mentioned (highest yield/conversion).\n"
                "   - If no explicit 'best' is labeled, infer it based on the highest reported metrics.\n"
                "   - Look for temperature, residence time, flow rates, and reactor details associated with that result.\n"
                "3. Extract all reactants and products involved in this main reaction.\n\n"
                "NEGATIVE CONSTRAINTS: If the paragraph describes 'previous work', 'literature review', or 'background theory', ONLY extract data if it is explicitly compared with or part of the current study's experimental results. Otherwise, return a JSON with all nulls. Do NOT extract data cited from other papers unless relevant to the current study's optimization.\n\n"
                # "For conditions, prefer values with units (Â°C/K/Â°F; s/min/h; bar/MPa; mL/min). Ignore counts like '7 experiments'.\n"
                "Guidance:\n"
                "- For conditions, prefer values with units (Â°C/K/Â°F; s/min/h; bar/MPa; mL/min). Ignore counts like '7 experiments'.\n"
                "- If a result (yield/conversion) is mentioned, associate the nearest temperature/time in the same or preceding sentence.\n"
                "Schema:\n"
                "{ \"reaction_summary\": {\n"
                "  \"reaction_type\": <string|null>,\n"
                "  \"reactants\": [ {\"name\": <string>, \"role\": \"reactant\"|\"catalyst\"|\"solvent\"} ],\n"
                "  \"products\": [ {\"name\": <string>, \"yield_optimal\": <number|null>, \"unit\": \"%\"} ],\n"
                "  \"conditions\": [\n"
                "    {\"type\": \"temperature\", \"value\": <string>},\n"
                "    {\"type\": \"residence_time\", \"value\": <string>},\n"
                "    {\"type\": \"flow_rate_total\", \"value\": <string>},\n"
                "    {\"type\": \"pressure\", \"value\": <string>}\n"
                "  ],\n"
                "  \"reactor\": {\"type\": <string>, \"inner_diameter\": <string>},\n"
                "  \"metrics\": {\"conversion\": <number|null>, \"yield\": <number|null>, \"selectivity\": <number|null>, \"unit\": \"%\"}\n"
                "} }\n"
                "Rules:\n"
                "- Keep original units (e.g. '100 Â°C', '30 min').\n"
                "- Map flow rates to 'flow_rate_total' if unspecified.\n"
                "- Extract full chemical names where possible.\n"
            )
        summarized = []
        used_col = 'content' if 'content' in df.columns else ('abstract' if 'abstract' in df.columns else 'content')
        total_cnt = 0
        parsed_ok_cnt = 0
        ellipsis_cnt = 0
        signal0_cnt = 0
        for _, row in df.iterrows():
            # ä¼˜å…ˆä½¿ç”¨è¿‡æ»¤åçš„åŸæ–‡æ®µè½ï¼ˆcontentï¼‰ï¼›è‹¥æ— åˆ™ä½¿ç”¨ abstract
            content = str(row.get(used_col, row.get('content', row.get('abstract', ''))))
            content = self._clip(content, 2200)
            
            # ä½¿ç”¨æ–°çš„ _get_chat_prompt æ–¹æ³•ï¼Œè‡ªåŠ¨é€‰æ‹©æ ¼å¼
            base_prompt = self._get_chat_prompt(system_prompt, user_prompt, content, stage='summarize')
            
            model = self._get_stage_model('summarize')
            # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´ max_tokensï¼šå¾®è°ƒæ¨¡å‹ç»™è¶³ 2048ï¼Œæœªå¾®è°ƒæ¨¡å‹é™åˆ¶åœ¨ 800 ä»¥å…æŒ¤å  Input Context
            gen_max_tokens = 2048 if use_finetuned else 800
            raw = self._safe_generate(model, base_prompt, max_tokens=gen_max_tokens, temp=0.0)
            
            # å»å›´æ å¹¶æŠ½å–èŠ±æ‹¬å·å—
            import re as _re
            raw = _re.sub(r"```json\s*", "", raw, flags=_re.IGNORECASE)
            raw = _re.sub(r"```\s*", "", raw)
            start, end = raw.find('{'), raw.rfind('}')
            if start != -1 and end != -1 and end > start:
                raw = raw[start:end+1]
            txt = self._sanitize_json_text(raw)
            txt = self._keep_best_only(txt)
            # æ®µè½åå¤„ç†éªŒè¯ï¼šå¼ºç±»å‹ä¸åŸæ–‡å¯¹é½
            def _post_validate_json(json_text: str, paragraph: str) -> str:
                import json as _json, re as _re
                try:
                    obj = _json.loads(json_text)
                except Exception:
                    return json_text
                para = paragraph or ""
                para_low = para.lower()
                # 1) äº§å“åç§°å¼ºç±»å‹ï¼šæ•°å­—/ç¼–å·ä½œå â†’ null
                prods = obj.get("products", []) or []
                fixed_prods = []
                for p in prods if isinstance(prods, list) else []:
                    if not isinstance(p, dict):
                        continue
                    name = p.get("name")
                    # çº¯æ•°å­—æˆ–ç¼–å·æ ·å¼ä¸ä½œä¸ºåˆæ³•åç§°
                    if isinstance(name, (int, float)):
                        p["name"] = None
                    elif isinstance(name, str):
                        n = name.strip()
                        if _re.fullmatch(r"\(?\d+\)?\.?", n):
                            p["name"] = None
                    fixed_prods.append(p)
                obj["products"] = fixed_prods
                # 2) æ•°å€¼åŸæ–‡å¯¹é½ï¼šmetrics ä¸ yield_optimal å¿…é¡»åœ¨æ®µè½ä¸­å‡ºç°
                def _num_in_text(val: float) -> bool:
                    # æ„é€ è‹¥å¹²åŒ¹é…å½¢å¼ï¼š99.5ã€99.5%ï¼ˆå…è®¸ç©ºæ ¼ï¼‰
                    s = f"{val}".rstrip('0').rstrip('.') if isinstance(val, float) else f"{val}"
                    patterns = [
                        rf"\b{s}\b",
                        rf"\b{s}\s*%\b",
                    ]
                    for pat in patterns:
                        if _re.search(pat, para):
                            return True
                    return False
                met = obj.get("metrics") or {}
                if not isinstance(met, dict):
                    met = {}
                for k in ["conversion", "yield", "selectivity"]:
                    v = met.get(k)
                    if isinstance(v, (int, float)):
                        if not _num_in_text(v):
                            met[k] = None
                obj["metrics"] = met
                # yield_optimal æ ¡éªŒ
                prods2 = obj.get("products", []) or []
                for p in prods2:
                    y = p.get("yield_optimal")
                    if isinstance(y, (int, float)):
                        if not _num_in_text(y):
                            p["yield_optimal"] = None
                obj["products"] = prods2
                # 3) æ¡ä»¶å€¼å‡ºç°æ€§ï¼šæ”¾å®½æ ¡éªŒ
                # åŸå› ï¼šLLM å¯èƒ½ä¼šå°† "25 Â°C" æ ¼å¼åŒ–ä¸º "25Â°C"ï¼Œæˆ–è€…å°† "room temperature" æå–ä¸º "25 Â°C"ã€‚
                # è¿‡äºä¸¥æ ¼çš„åŸæ–‡åŒ¹é…ä¼šå¯¼è‡´å¤§é‡æœ‰æ•ˆä¿¡æ¯ä¸¢å¤±ã€‚
                conds = obj.get("conditions", []) or []
                fixed_conds = []
                # æ¡ä»¶å€¼å•ä½/çŸ­è¯­æ ¡éªŒï¼šå…è®¸å•ä½æˆ–å¸¸è§çŸ­è¯­ï¼ˆroom temperature/RTï¼‰ï¼Œå¦åˆ™ç½® None
                _unit_re = _re.compile(
                    r"(?:\b(?:Â°C|K|Â°F)\b|\b\d+\.?\d*\s*(?:min|s|h)\b|\b(?:bar|MPa)\b|\bmL\s*/\s*(?:min|h)\b|\bÂµL\s*/\s*min\b|\buL\s*/\s*min\b)",
                    _re.IGNORECASE,
                )
                for c in conds if isinstance(conds, list) else []:
                    if not isinstance(c, dict):
                        continue
                    val = c.get("value")
                    if isinstance(val, str):
                        v = val.strip()
                        if not v:
                            c["value"] = None
                        else:
                            low = v.lower()
                            if ("room temperature" in low) or (low == "rt"):
                                c["value"] = v
                            elif _unit_re.search(v):
                                c["value"] = v
                            else:
                                c["value"] = None
                    else:
                        c["value"] = None
                    fixed_conds.append(c)
                obj["conditions"] = fixed_conds
                # 4) ååº”ç±»å‹å½’ä¸€åŒ–ä¸æœ€ç»ˆ schema/é¡ºåº
                obj = self._ensure_schema_and_order(obj)
                try:
                    return _json.dumps(obj, ensure_ascii=False)
                except Exception:
                    return json_text
            # ä¸¥æ ¼åå¤„ç†å¼€å…³ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ FCPD_POST_STRICT=0 å¯è·³è¿‡ä¸¥æ ¼æ ¡éªŒï¼Œä»¥ä¾¿è¯„æµ‹æ”¾å®½
            if os.getenv("FCPD_POST_STRICT", "1") != "0":
                txt = _post_validate_json(txt, content)
            # éª¨æ¶ JSONï¼ˆç”¨äºè§£æå¤±è´¥æˆ–å¼‚å¸¸æ—¶å…œåº•ï¼‰
            def _skeleton_json() -> str:
                import json as _json
                return _json.dumps({
                    "reaction_summary": {"reaction_type": None},
                    "reactants": [],
                    "products": [],
                    "conditions": [
                        {"type": "temperature", "value": None},
                        {"type": "residence_time", "value": None},
                        {"type": "flow_rate_total", "value": None},
                        {"type": "pressure", "value": None}
                    ],
                    "reactor": {"type": None, "inner_diameter": None},
                    "metrics": {"conversion": None, "yield": None, "selectivity": None, "unit": "%"}
                }, ensure_ascii=False)
            # ç»Ÿè®¡ä¸æ”¾å®½è¿‡æ»¤ï¼šä»…â€œä¿¡å·=0â€æ—¶ä¸¢å¼ƒï¼›'...' ä»…è®¡æ•°ä¸ç›´æ¥ä¸¢å¼ƒï¼›è§£æå¤±è´¥æ‰ä¸¢å¼ƒ
            total_cnt += 1
            if '...' in txt or 'â€¦' in txt or '...' in raw or 'â€¦' in raw:
                ellipsis_cnt += 1
            try:
                import json as _json
                obj = _json.loads(txt)
                parsed_ok_cnt += 1
            except Exception:
                summarized.append(_skeleton_json())  # è§£æå¤±è´¥è¾“å‡ºéª¨æ¶
                continue
            # ç»Ÿè®¡æœ‰æ•ˆä¿¡å·
            signals = 0
            # reactants
            for r in obj.get('reactants', []) or []:
                if isinstance(r, dict) and r.get('name'):
                    signals += 1
                    break
            # products
            for p in obj.get('products', []) or []:
                if isinstance(p, dict) and (p.get('name') or isinstance(p.get('yield_optimal'), (int, float))):
                    signals += 1
                    break
            # conditions
            for c in obj.get('conditions', []) or []:
                if isinstance(c, dict) and c.get('value'):
                    signals += 1
                    break
            # reactor
            reactor = obj.get('reactor') or {}
            if isinstance(reactor, dict) and (reactor.get('type') or reactor.get('inner_diameter')):
                signals += 1
            # metrics
            met = obj.get('metrics') or {}
            if isinstance(met, dict) and any(isinstance(met.get(k), (int, float)) for k in ['conversion','yield','selectivity']):
                signals += 1
            if signals == 0:
                signal0_cnt += 1
            try:
                import json as _json
                summarized.append(_json.dumps(obj, ensure_ascii=False))
            except Exception:
                summarized.append(txt if signals >= 0 else _skeleton_json())
        out = df.copy()
        out['summarized'] = summarized
        # è¯Šæ–­å¯è§‚æµ‹æ€§ï¼ˆä»…åœ¨è°ƒè¯•æ¨¡å¼è¾“å‡ºï¼‰
        if os.getenv("FCPD_DEBUG", "0") == "1":
            try:
                print(f"  ğŸ” Summarizeç»Ÿè®¡ -> æ®µè½æ¥æºåˆ—: {used_col}, æ®µè½æ€»æ•°: {total_cnt}, è§£ææˆåŠŸ: {parsed_ok_cnt}, å«çœç•¥å·: {ellipsis_cnt}, ä¿¡å·=0æ¡æ•°: {signal0_cnt}")
            except Exception:
                pass
        return out

    def summarize_document_overall(self, df_abstract: pd.DataFrame) -> str:
        """å°†å¤šä¸ªæ®µè½çº§ç»“æ„åŒ–JSONæ±‡æ€»ä¸ºå•ä¸€æœ€ä½³JSONï¼›è‹¥æ— JSONåˆ™å›è½åˆ°åŸºäºæ–‡æœ¬çš„æ±‡æ€»ã€‚"""
        import re, json
        # é¢„ç®—ä¸åˆ†æ‰¹å‚æ•°ï¼ˆé¿å…ä¸Šä¸‹æ–‡æº¢å‡ºï¼‰
        max_candidates = int(os.getenv('FCPD_OVERALL_MAX_CAND', '24'))
        char_budget = int(os.getenv('FCPD_OVERALL_CHAR_BUDGET', '8000'))
        chunk_size = int(os.getenv('FCPD_OVERALL_CHUNK_SIZE', '12'))
        overall_max_tokens = int(os.getenv('FCPD_OVERALL_MAX_TOKENS', '450'))

        def _select_under_budget(items: list[str]) -> list[str]:
            selected = []
            total = 0
            for s in items:
                if len(selected) >= max_candidates:
                    break
                add_len = len(s) + (2 if selected else 0)
                if (total + add_len) > char_budget:
                    break
                selected.append(s)
                total += add_len
            return selected

        def _merge_once(cands: list[str], system_prompt: str, use_ft_overall: bool, user_prompt_str: str) -> str:
            nonlocal overall_max_tokens
            if not cands:
                return ""
            # æŒ‰é¢„ç®—è£å‰ªå€™é€‰
            cands = _select_under_budget(cands)
            combined_local = "\n\n".join(cands)
            # æ„å»ºæç¤ºè¯
            user_prompt = user_prompt_str
            prompt_local = self._get_chat_prompt(system_prompt, user_prompt, combined_local, stage='overall')
            raw_local = self._safe_generate(self._get_stage_model('overall'), prompt_local, max_tokens=overall_max_tokens, temp=0.0)
            raw_local = re.sub(r"```json\s*", "", raw_local, flags=re.IGNORECASE)
            raw_local = re.sub(r"```\s*", "", raw_local)
            s0, e0 = raw_local.find("{"), raw_local.rfind("}")
            if s0 != -1 and e0 != -1 and e0 > s0:
                raw_local = raw_local[s0:e0+1]
            return self._keep_best_only(self._sanitize_json_text(raw_local))

        # ä¼˜å…ˆä½¿ç”¨æ®µè½çº§JSONï¼ˆåˆ—åä¸º summarizedï¼‰
        if 'summarized' in df_abstract.columns:
            # ä»…æ”¶é›†åŒ…å«èŠ±æ‹¬å·çš„å€™é€‰ JSON è¡Œï¼Œé€æ¡æ‹¼æ¥åˆ° Context
            raw_items = df_abstract['summarized'].fillna("").tolist()
            json_items = []
            # é¢„æ”¶é›†å€™é€‰ä¸­çš„æ¡ä»¶å€¼ï¼Œç”¨äºâ€œå”¯ä¸€ä¸€è‡´å€¼â€è¡¥å…¨
            cond_pool = {"temperature": set(), "residence_time": set(), "flow_rate_total": set(), "pressure": set()}
            def _valid_cond_value(v: str) -> bool:
                try:
                    import re as _re
                    if not isinstance(v, str) or not v.strip():
                        return False
                    low = v.strip().lower()
                    if ("room temperature" in low) or (low == "rt"):
                        return True
                    unit_re = _re.compile(r"(?:\b(?:Â°C|K|Â°F)\b|\b\d+\.?\d*\s*(?:min|s|h)\b|\b(?:bar|MPa)\b|\bmL\s*/\s*(?:min|h)\b|\bÂµL\s*/\s*min\b|\buL\s*/\s*min\b)", _re.IGNORECASE)
                    return bool(unit_re.search(v))
                except Exception:
                    return False
            def _collect_cond_values(txt: str) -> None:
                import json as _json
                try:
                    obj = _json.loads(txt)
                except Exception:
                    return
                conds = obj.get("conditions") or obj.get("reaction_summary", {}).get("conditions") or []
                if isinstance(conds, list):
                    for c in conds:
                        if isinstance(c, dict):
                            t = c.get("type")
                            v = c.get("value")
                            if t in cond_pool and _valid_cond_value(v):
                                cond_pool[t].add(v.strip())
            for t in raw_items:
                s = str(t).strip()
                if not s:
                    continue
                if "{" in s and "}" in s:
                    json_items.append(s)
                    _collect_cond_values(s)
            for t in raw_items:
                s = str(t).strip()
                if not s:
                    continue
                if "{" in s and "}" in s:
                    json_items.append(s)
            # ä¸ºé˜²æ­¢è¶…é•¿ï¼Œé™åˆ¶å€™é€‰æ¡æ•°ä¸å­—ç¬¦é¢„ç®—
            combined_items = _select_under_budget(json_items)
            combined = "\n\n".join(combined_items)
            if not json_items:
                # è‹¥ summarized å­˜åœ¨ä½†å‡ä¸ºç©º/æ— æ•ˆï¼Œè§¦å‘å›è½
                col = 'abstract' if 'abstract' in df_abstract.columns else 'content'
                texts = [t for t in df_abstract[col].fillna("").tolist() if t.strip()]
                combined = "\n\n".join(texts)
                if len(combined) > char_budget:
                    combined = combined[:char_budget]
                system_prompt = "You output ONLY valid JSON. No explanations."
                user_prompt = (
                    "Extract ONE JSON summarizing the optimal condition set from the document text. "
                    "Use null/empty arrays for unknowns. Do not invent chemicals or numbers not present in text. "
                    "If conflicting info exists, choose null. Keep metrics numeric when present. "
                    "Schema keys must be exactly: reaction_summary.reaction_type, reactants[{name,role}], "
                    "products[{name,yield_optimal,unit}], conditions[{type,value}], reactor{type,inner_diameter}, "
                    "metrics{conversion,yield,selectivity,unit}."
                )
            else:
                # å¯¹å¾®è°ƒæ¨¡å‹ä½¿ç”¨æ›´ç®€æ´ä¸”æ˜ç¡®çš„åˆå¹¶æç¤ºè¯
                use_ft_overall = False
                try:
                    if self.model_overall_name and self._is_finetuned_model(self.model_overall_name):
                        use_ft_overall = True
                    elif self.model_summarize_name and self._is_finetuned_model(self.model_summarize_name):
                        use_ft_overall = True
                except Exception:
                    use_ft_overall = False
                system_prompt = "You output ONLY valid JSON. No explanations."
                if use_ft_overall:
                    user_prompt_template = (
                        "You are given multiple JSON candidates (one per paragraph). "
                        "Merge them into ONE JSON following this schema strictly (keys in this exact order):\n"
                        "1) reaction_summary.reaction_type\n"
                        "2) reactants[{name,role}]\n"
                        "3) products[{name,yield_optimal,unit}]\n"
                        "4) conditions[{type,value}]  (types: temperature,residence_time,flow_rate_total,pressure)\n"
                        "5) reactor{type,inner_diameter}\n"
                        "6) metrics{conversion,yield,selectivity,unit}\n\n"
                        "CRITICAL FILTERING RULES (Focus on Main Reaction):\n"
                        "- Identify the MAIN Target Reaction of this specific study/paper. \n"
                        "- EXCLUDE chemicals and data clearly labeled as \"previous work\", \"reported by others\", \"literature precedent\", or \"background\".\n"
                        "- Only aggregate reactants/products belonging to the current study's experiments.\n\n"
                        "MERGING RULES:\n"
                        "- Reactants & Products: Collect ALL unique substances involved in the MAIN reaction (reactants, solvents, catalysts). Merge duplicates by name.\n"
                        "- Metrics: Find the candidate with the HIGHEST yield (or conversion).\n"
                        "- Conditions: You MUST prioritize extracting the condition set (temp, time, etc.) FROM THE SAME paragraph/candidate that provided the highest yield. \n"
                        "- IF and ONLY IF the best yield paragraph lacks specific conditions (e.g., temperature/time is null), you MAY infer them from other paragraphs describing the SAME reaction setup or \"general procedure\".\n"
                        "- Reactor: choose the most informative object describing the main reactor.\n"
                        "- Do NOT invent new data. Output valid JSON only.\n\n"
                        "Use ONLY the candidates provided in Context.\n"
                    )
                    # è‹¥å€™é€‰è¿‡å¤šï¼Œæ‰§è¡Œåˆ†æ‰¹ä¸¤æ®µå¼åˆå¹¶
                    items_for_merge = combined_items
                    if len(json_items) > max_candidates:
                        partials = []
                        for i in range(0, len(json_items), chunk_size):
                            chunk = json_items[i:i+chunk_size]
                            partial = _merge_once(chunk, system_prompt, True, user_prompt_template)
                            if partial:
                                partials.append(partial)
                        # ç¬¬äºŒé˜¶æ®µï¼šåˆå¹¶åˆ†æ‰¹ç»“æœ
                        if partials:
                            items_for_merge = partials
                    # ä¸€æ¬¡åˆå¹¶ï¼ˆå¯¹é¢„ç®—å†…çš„å€™é€‰æˆ–åˆ†æ‰¹ä¸­é—´ç»“æœï¼‰
                    combined = "\n\n".join(_select_under_budget(items_for_merge))
                    user_prompt = user_prompt_template
                else:
                    user_prompt = (
                        "From the following JSON candidates, output ONE JSON that keeps the optimal condition set "
                        "(highest yield/conversion). Use null/empty arrays for unknowns. "
                        "Do NOT introduce chemicals/conditions/metrics not present in candidates. "
                        "If conflicting info exists, choose null. Metrics must be numeric when present. "
                        "Schema keys must be exactly: reaction_summary.reaction_type, reactants[{name,role}], "
                        "products[{name,yield_optimal,unit}], conditions[{type,value}], reactor{type,inner_diameter}, "
                        "metrics{conversion,yield,selectivity,unit}."
                    )
        else:
            # å›è½ï¼šåŸºäºæŠ½è±¡/åŸæ–‡è¿›è¡Œä¸€æ¬¡æ•´ä½“æŠ½å–
            col = 'abstract' if 'abstract' in df_abstract.columns else 'content'
            texts = [t for t in df_abstract[col].fillna("").tolist() if t.strip()]
            combined = "\n\n".join(texts)
            if len(combined) > char_budget:
                combined = combined[:char_budget]
            system_prompt = "You output ONLY valid JSON. No explanations."
            user_prompt = (
                "Extract ONE JSON summarizing the optimal condition set from the document text. "
                "Use null/empty arrays for unknowns. Do not invent chemicals or numbers not present in text. "
                "If conflicting info exists, choose null. Keep metrics numeric when present. "
                "Schema keys must be exactly: reaction_summary.reaction_type, reactants[{name,role}], "
                "products[{name,yield_optimal,unit}], conditions[{type,value}], reactor{type,inner_diameter}, "
                "metrics{conversion,yield,selectivity,unit}."
            )
        # ä½¿ç”¨ overall é˜¶æ®µæ¨¡å‹ä¸å¯¹åº”çš„ chat templateï¼ˆè‹¥ä¸ºå¾®è°ƒæ¨¡å‹ï¼‰
        prompt = self._get_chat_prompt(system_prompt, user_prompt, combined, stage='overall')
        raw = self._safe_generate(self._get_stage_model('overall'), prompt, max_tokens=overall_max_tokens, temp=0.0)
        # å»å›´æ å¹¶æŠ½å–æœ€å¤§èŠ±æ‹¬å·å—
        raw = re.sub(r"```json\s*", "", raw, flags=re.IGNORECASE)
        raw = re.sub(r"```\s*", "", raw)
        s, e = raw.find("{"), raw.rfind("}")
        if s != -1 and e != -1 and e > s:
            raw = raw[s:e+1]
        cleaned = self._sanitize_json_text(raw)
        cleaned = self._keep_best_only(cleaned)
        
        # ---------------------------------------------------------------------
        # (å·²ç§»é™¤) å¼ºåŠ›åå¤„ç†ï¼šç¡¬ç¼–ç åˆå¹¶
        # åŸå› ï¼šä¸ºäº†å“åº”â€œåªåˆå¹¶ä¸»ååº”â€çš„éœ€æ±‚ï¼Œæˆ‘ä»¬ç°åœ¨å®Œå…¨ä¾èµ– Prompt è¿›è¡Œæ™ºèƒ½è¿‡æ»¤ï¼Œ
        # ä¸å†æ— è„‘åˆå¹¶æ‰€æœ‰å€™é€‰ï¼Œä»¥å…å¼•å…¥èƒŒæ™¯ä»‹ç»ä¸­çš„å™ªéŸ³ã€‚
        # ---------------------------------------------------------------------
        
        try:
            final_obj = json.loads(cleaned)
        except:
            final_obj = {}

        # è‡ªåŠ¨ä¿®æ­£ï¼šæ ¹æ® role å­—æ®µè°ƒæ•´å½’å±
        try:
            obj = json.loads(cleaned)
            obj = self._ensure_schema_and_order(obj)
            cleaned = json.dumps(obj, ensure_ascii=False)
        except Exception:
            pass
        def _looks_invalid(text: str) -> bool:
            if not text or '...' in text or 'â€¦' in text:
                return True
            try:
                json.loads(text)
                return False
            except Exception:
                return True
        if _looks_invalid(cleaned):
            # ä¸€æ¬¡æ›´ä¸¥æ ¼é‡è¯•ï¼šå¼ºè°ƒä»…åŸºäºå€™é€‰/æ–‡æœ¬ä¸”å¿…é¡»å¯è§£æ
            strict_user = user_prompt + " STRICT MODE: Use only fields present in candidates/text; ensure valid, parseable JSON; no markdown or code fences."
            prompt2 = self._get_chat_prompt(system_prompt, strict_user, combined, stage='overall')
            raw2 = self._safe_generate(self._get_stage_model('overall'), prompt2, max_tokens=overall_max_tokens, temp=0.0)
            raw2 = re.sub(r"```json\s*", "", raw2, flags=re.IGNORECASE)
            raw2 = re.sub(r"```\s*", "", raw2)
            s2, e2 = raw2.find("{"), raw2.rfind("}")
            if s2 != -1 and e2 != -1 and e2 > s2:
                raw2 = raw2[s2:e2+1]
            cleaned2 = self._keep_best_only(self._sanitize_json_text(raw2))
            try:
                obj2 = json.loads(cleaned2)
                obj2 = self._ensure_schema_and_order(obj2)
                return json.dumps(obj2, ensure_ascii=False)
            except Exception:
                return raw2
        try:
            objf = json.loads(cleaned)
            objf = self._ensure_schema_and_order(objf)
            
            # è½»é‡â€œè§’è‰²å›æ”¶â€ï¼šè‹¥æŸç‰©è´¨åœ¨å€™é€‰ä¸­é«˜é¢‘ä½œä¸ºæº¶å‰‚ä¸”åœ¨å½“å‰ products æ— äº§ç‡ï¼Œåˆ™ç§»å› reactants
            try:
                # 1. ç»Ÿè®¡å€™é€‰ä¸­å„ç‰©è´¨ä½œä¸º solvent çš„æ¬¡æ•°
                solvent_counts = {}
                for item in json_items:
                    try:
                        c_obj = json.loads(item)
                        c_reactants = c_obj.get("reactants") or c_obj.get("reaction_summary", {}).get("reactants") or []
                        if isinstance(c_reactants, list):
                            for r in c_reactants:
                                if isinstance(r, dict):
                                    name = self._clean_chem_name(r.get("name"))
                                    role = str(r.get("role", "")).lower()
                                    if name and "solvent" in role:
                                        solvent_counts[name] = solvent_counts.get(name, 0) + 1
                    except Exception:
                        continue
                
                # 2. æ£€æŸ¥ products å¹¶è¿ç§»
                final_prods = objf.get("products") or []
                final_reacts = objf.get("reactants") or []
                new_prods = []
                dirty = False
                
                for p in final_prods:
                    if not isinstance(p, dict):
                        continue
                    p_name = self._clean_chem_name(p.get("name"))
                    p_yield = p.get("yield_optimal")
                    
                    # è§¦å‘æ¡ä»¶ï¼šåå­—åŒ¹é…æº¶å‰‚é«˜é¢‘è¯ï¼ˆ>=2æ¬¡ï¼‰ä¸”æ— äº§ç‡
                    if p_name and p_yield is None and solvent_counts.get(p_name, 0) >= 2:
                        # è¿ç§»åˆ° reactants
                        # æŸ¥é‡ï¼šreactants é‡Œæ˜¯å¦å·²å­˜åœ¨
                        exists = any(self._clean_chem_name(r.get("name")) == p_name for r in final_reacts)
                        if not exists:
                            final_reacts.append({"name": p_name, "role": "solvent"})
                            dirty = True
                    else:
                        new_prods.append(p)
                
                if dirty:
                    objf["products"] = new_prods
                    objf["reactants"] = final_reacts
            except Exception:
                pass

            # è½»é‡æ¡ä»¶è¡¥å…¨ï¼šå½“æœ€ä½³æ”¶ç›ŠJSONç¼ºæŸæ¡ä»¶ï¼Œä¸”å€™é€‰å”¯ä¸€ä¸€è‡´æ—¶è¿›è¡Œè¡¥å…¨
            if os.getenv('FCPD_OVERALL_COND_FILL', '1') == '1':
                try:
                    # cond_pool ä»…åœ¨ summarized åˆ†æ”¯ä¸­æ„å»º
                    if 'cond_pool' in locals():
                        existing = objf.get("conditions") or []
                        # æ„å»ºç±»å‹åˆ°ç´¢å¼•æ˜ å°„
                        idx_map = {}
                        for idx, c in enumerate(existing if isinstance(existing, list) else []):
                            if isinstance(c, dict) and c.get("type") in ("temperature","residence_time","flow_rate_total","pressure"):
                                idx_map[c.get("type")] = idx
                        for t in ("temperature","residence_time","flow_rate_total","pressure"):
                            pool = cond_pool.get(t, set())
                            if isinstance(pool, set) and len(pool) == 1:
                                val = list(pool)[0]
                                # å¦‚æœå½“å‰ç¼ºå¤±åˆ™è¡¥å…¨
                                if t in idx_map:
                                    if not existing[idx_map[t]].get("value"):
                                        existing[idx_map[t]]["value"] = val
                                else:
                                    if isinstance(existing, list):
                                        existing.append({"type": t, "value": val})
                        objf["conditions"] = existing
                except Exception:
                    pass
            return json.dumps(objf, ensure_ascii=False)
        except Exception:
            return raw

    def _extract_influence_candidates(self, df: pd.DataFrame) -> list:
        import re, os
        content_col = 'abstract' if 'abstract' in df.columns else 'content'
        candidates = []
        topK = int(os.getenv('FCPD_IMPACT_TOPK', '12'))
        
        causal_verbs = [
            'increase', 'decrease', 'improve', 'enhance', 'reduce', 'affect',
            'influence', 'impact', 'promote', 'inhibit', 'facilitate', 'optimize',
            'control', 'determine', 'depend', 'vary', 'change', 'modulate'
        ]
        metric_keywords = [
            'conversion', 'yield', 'selectivity', 'purity', 'efficiency',
            'product distribution', 'heat transfer', 'mixing'
        ]
        
        for idx, row in df.iterrows():
            paragraph = str(row[content_col])
            if not paragraph or len(paragraph.strip()) < 30:
                continue
            score = 0
            para_lower = paragraph.lower()
            
            # Results/Discussion sections
            is_results = any(sec in para_lower for sec in ['result', 'discussion', 'finding', 'observation'])
            if is_results:
                has_metric = any(kw in para_lower for kw in metric_keywords)
                if has_metric:
                    score += 25
                else:
                    score += 10
            
            # å› æœåŠ¨è¯
            causal_count = sum(1 for verb in causal_verbs if verb in para_lower)
            if causal_count > 0:
                has_metric = any(m in para_lower for m in metric_keywords)
                if has_metric:
                    score += 8 * min(causal_count, 3)
                else:
                    score += 3 * min(causal_count, 2)
            
            # å®šé‡æ•°æ®
            quant_matches = re.findall(r'\b\d+\.?\d*\s*(?:%|K|Â°C|Â°F|mL|L|min|h|MPa|bar|M|mol|g)\b', paragraph)
            if len(quant_matches) >= 2:
                score += 10
            
            if score >= 3:  # è¿›ä¸€æ­¥æ”¾å®½é˜ˆå€¼ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿå€™é€‰
                candidates.append({'index': idx, 'text': paragraph, 'score': score})
        
        candidates.sort(key=lambda x: x['score'], reverse=True)
        return candidates[:topK]

    def _to_markdown_impact(self, items: list) -> str:
        if not items:
            return "| Factor | Metric | Direction |\n|--------|--------|-----------|\n| None | - | - |"
        lines = [
            "| Factor | Metric | Direction |",
            "|--------|--------|-----------|",
        ]
        for it in items:
            factor = (it.get('factor','-') or '-').replace('|','\\|')
            metric = (it.get('metric','-') or '-').replace('|','\\|')
            direction = (it.get('direction','') or '').lower()
            if direction not in ['increase','decrease','unchanged']:
                direction = '-'
            lines.append(f"| {factor} | {metric} | {direction} |")
        return "\n".join(lines)

    def extract_influence_factors_with_llm(self, df: pd.DataFrame) -> str:
        cands = self._extract_influence_candidates(df)
        if os.getenv("FCPD_DEBUG", "0") == "1":
            print(f"  ğŸ› Impactå€™é€‰æ®µè½æ•°: {len(cands)}")
        if not cands:
            return self._to_markdown_impact([])
        joined = "\n\n".join(c['text'][:800] for c in cands[:min(len(cands), 15)])
        if os.getenv("FCPD_DEBUG", "0") == "1":
            print(f"  ğŸ› æ‹¼æ¥æ–‡æœ¬é•¿åº¦: {len(joined)} å­—ç¬¦")

        # Few-shot ç®€åŒ–ä½†å®Œæ•´çš„æ¡ˆä¾‹
        system_prompt = "Extract cause-effect relationships from chemical paragraphs."
        user_prompt = (
            "Example paragraph:\n"
            "Residence time is an important parameter. A longer residence time will result in "
            "a higher conversion of TFMB. The product selectivity nearly remains unchanged.\n\n"
            "Example output:\n"
            "residence_time | conversion of TFMB | increase\n"
            "residence_time | product selectivity | unchanged\n\n"
            "Extract Factor-Metric-Direction from the paragraphs below.\n"
            "Format: Factor | Metric | Direction (one per line, no table headers)\n"
            "Direction: increase, decrease, or unchanged\n\n"
            "Paragraphs:\n"
        )
        prompt = self._create_prompt(system_prompt, user_prompt, joined)
        # å½±å“å› ç´ ï¼šæ”¹ä¸ºä½¿ç”¨ impact ä¸“ç”¨æ¨¡å‹ï¼ˆè‹¥å·²é…ç½®ï¼‰ï¼Œå¦åˆ™å›é€€åˆ° summarize é˜¶æ®µæ¨¡å‹
        model = self._get_stage_model('impact')
        raw = self._safe_generate(model, prompt, max_tokens=700, temp=0.3)
        if os.getenv("FCPD_DEBUG", "0") == "1":
            print(f"  ğŸ› LLMåŸå§‹è¾“å‡ºå‰500å­—ç¬¦: {raw[:500]}")

        # è§£æä¸ºä¸‰åˆ—ï¼ˆå…¼å®¹2åˆ—å’Œ3åˆ—æ ¼å¼ï¼‰
        items = []
        for line in raw.splitlines():
            if '|' not in line:
                continue
            # è·³è¿‡è¡¨å¤´è¡Œ
            low = line.lower()
            if 'factor' in low and 'metric' in low:
                continue
            parts = [p.strip() for p in line.split('|')]
            if len(parts) < 2:  # æ”¹ä¸ºè‡³å°‘2åˆ—å³å¯
                continue
            # è·³è¿‡ç©ºè¡Œ
            if not parts[0] or not parts[1] or parts[0] == '-':
                continue
            
            # å…¼å®¹2åˆ—å’Œ3åˆ—æ ¼å¼
            if len(parts) == 2:
                # ä¸¤åˆ—æ ¼å¼ï¼šFactor | Metricï¼ˆä»Metricä¸­æå–Directionï¼‰
                factor = parts[0]
                metric_with_dir = parts[1].lower()
                # å°è¯•ä»metricå­—æ®µæå–æ–¹å‘
                if 'increase' in metric_with_dir or 'higher' in metric_with_dir:
                    direction = 'increase'
                    metric = parts[1].split()[0]  # å–ç¬¬ä¸€ä¸ªè¯ä½œä¸ºmetric
                elif 'decrease' in metric_with_dir or 'lower' in metric_with_dir:
                    direction = 'decrease'
                    metric = parts[1].split()[0]
                elif 'unchange' in metric_with_dir:
                    direction = 'unchanged'
                    metric = parts[1].split()[0]
                else:
                    direction = '-'
                    metric = parts[1]
            else:
                # ä¸‰åˆ—æ ¼å¼ï¼šFactor | Metric | Direction
                factor = parts[0]
                metric = parts[1]
                direction = parts[2].lower()
                if 'increase' in direction or 'higher' in direction or 'improve' in direction or 'enhance' in direction:
                    direction = 'increase'
                elif 'decrease' in direction or 'lower' in direction or 'reduce' in direction or 'inhibit' in direction:
                    direction = 'decrease'
                elif 'unchange' in direction or 'unchanged' in direction or 'no effect' in direction:
                    direction = 'unchanged'
                else:
                    direction = ''
            
            items.append({'factor': factor, 'metric': metric, 'direction': direction})
        return self._to_markdown_impact(items)

    def process_text_file_comprehensive(self, file_path: str, mode: str = 'comprehensive') -> Dict[str, Any]:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        segs, cur = [], []
        for line in lines:
            if line.strip():
                cur.append(line.strip())
            else:
                if cur:
                    segs.append(' '.join(cur))
                    cur = []
        if cur:
            segs.append(' '.join(cur))
        df = pd.DataFrame(segs, columns=['content'])

        outputs: Dict[str, Any] = {}
        if mode in ['filter', 'comprehensive']:
            df_filtered = self.filter_content(df)
            filter_file = file_path.replace('.txt', '_Filtered.txt')
            self.save_df_to_text(df_filtered, filter_file)
            outputs['filter'] = filter_file
        if mode in ['abstract', 'comprehensive']:
            # ä¼˜åŒ–ï¼šè‹¥ä¸éœ€è¦ Impact æˆ– Impact æºé abstractï¼Œä¸”å·²æœ‰ filteredï¼Œåˆ™è·³è¿‡ abstract èŠ‚çœæ—¶é—´
            skip_abstract = False
            if mode == 'comprehensive':
                run_impact = os.getenv('FCPD_RUN_IMPACT', '1') == '1'
                impact_src = os.getenv('FCPD_IMPACT_SOURCE', 'summarized')
                # å¦‚æœ Impact å…³é—­ï¼Œæˆ–è€… Impact å¼€å¯ä½†æºä¸æ˜¯ abstractï¼Œåˆ™ä¸éœ€è¦è·‘ Abstract
                if (not run_impact) or (impact_src != 'abstract'):
                    skip_abstract = True
                    if os.getenv("FCPD_DEBUG", "0") == "1":
                        print("  â© è·³è¿‡ Abstract æ­¥éª¤ (ä¼˜åŒ–)")

            if not skip_abstract:
                df_abstract = self.abstract_text(df_filtered if 'df_filtered' in locals() else df)
                abstract_file = file_path.replace('.txt', '_Abstract.txt')
                self.save_df_to_text(df_abstract, abstract_file, 'abstract')
                outputs['abstract'] = abstract_file
        if mode in ['summarize', 'comprehensive']:
            # ä¼˜å…ˆä½¿ç”¨è¿‡æ»¤åçš„åŸæ–‡æ®µè½ï¼ˆcontentï¼‰è¿›è¡Œç»“æ„åŒ–æŠ½å–ï¼›è‹¥æ— åˆ™é€€åˆ° abstractï¼Œå†é€€åˆ°åŸå§‹ df
            df_input = (df_filtered if 'df_filtered' in locals() else (df_abstract if 'df_abstract' in locals() else df))
            # æŠ½å–å‰æ‰“åŒ…ï¼ˆå¯å¼€å…³ï¼‰ï¼šå°†ç›¸é‚»æ®µè½æŒ‰é¡ºåºåˆå¹¶åˆ°å­—ç¬¦é¢„ç®—èŒƒå›´ï¼Œæœ€å¤§åŒ–LLMä¸Šä¸‹æ–‡åˆ©ç”¨
            try:
                # æ™ºèƒ½ç›¸é‚»æ‰“åŒ…ï¼ˆé»˜è®¤å¼€å¯ï¼‰ï¼šä»…å¯¹ç–‘ä¼¼â€œæœ‰ç»“æœä½†ç¼ºæ¡ä»¶â€çš„è¡Œï¼Œä¸ç›¸é‚»Â±1è¡Œæ‹¼æ¥ï¼ˆé•¿åº¦â‰¤1500ï¼‰
                if os.getenv('FCPD_SUM_SMART_PACK', '1') == '1':
                    smart_max = int(os.getenv('FCPD_SUM_SMART_MAX_CHARS', '1500'))
                    smart_win = int(os.getenv('FCPD_SUM_SMART_WINDOW', '1'))
                    use_col = 'content' if 'content' in df_input.columns else ('abstract' if 'abstract' in df_input.columns else None)
                    if use_col:
                        df_input = self._smart_pack_rows(df_input, use_col, max_chars=smart_max, window=smart_win)
                if os.getenv('FCPD_SUM_PACK', '0') == '1':
                    min_chars = int(os.getenv('FCPD_SUM_PACK_MIN_CHARS', '2000'))
                    max_chars = int(os.getenv('FCPD_SUM_PACK_MAX_CHARS', '10000'))
                    use_col = 'content' if 'content' in df_input.columns else ('abstract' if 'abstract' in df_input.columns else None)
                    if use_col:
                        df_input = self._pack_adjacent_rows(df_input, use_col, min_chars=min_chars, max_chars=max_chars)
            except Exception:
                pass
            df_summarized = self.summarize_parameters(df_input)
            summarize_file = file_path.replace('.txt', '_Summarized.txt')
            self.save_df_to_text(df_summarized, summarize_file, 'summarized')
            outputs['summarized'] = summarize_file
            # Overallï¼ˆåŸºäºæŠ½è±¡ä¼˜å…ˆï¼‰
            # ä¼˜å…ˆä½¿ç”¨æ®µè½çº§JSONç»“æœï¼›è‹¥æ— åˆ™é€€å›åˆ°æŠ½è±¡ï¼Œå†é€€å›åŸå§‹è¾“å…¥
            overall_input = df_summarized if 'df_summarized' in locals() else (df_abstract if 'df_abstract' in locals() else df_input)
            overall_json = self.summarize_document_overall(overall_input)
            overall_file = file_path.replace('.txt', '_Overall.txt')
            with open(overall_file, 'w', encoding='utf-8') as f:
                f.write(overall_json)
            outputs['summarized_overall'] = overall_file

            # å½±å“å› ç´ ï¼ˆç®€åŒ–ä¸‰åˆ—ï¼‰
            # å½±å“å› ç´ ï¼ˆç®€åŒ–ä¸‰åˆ—ï¼‰
            try:
                if os.getenv('FCPD_RUN_IMPACT', '1') == '1':
                    src = os.getenv('FCPD_IMPACT_SOURCE', 'summarized')  # summarized | abstract | filtered | content
                    if src == 'abstract' and 'df_abstract' in locals():
                        impact_input = df_abstract
                    elif src == 'filtered' and 'df_filtered' in locals():
                        impact_input = df_filtered
                    elif src == 'summarized' and 'df_summarized' in locals():
                        impact_input = df_summarized
                    else:
                        impact_input = overall_input  # é»˜è®¤ä¸å½“å‰ä¸€è‡´ï¼ˆdf_summarizedä¼˜å…ˆï¼‰

                    impact_md = self.extract_influence_factors_with_llm(impact_input)
                    impact_file = file_path.replace('.txt', '_Impact_Analysis.txt')
                    with open(impact_file, 'w', encoding='utf-8') as f:
                        f.write("# Influence Factor Summary\n\n")
                        f.write(impact_md)
                    outputs['impact_analysis'] = impact_file
            except Exception:
                pass
        return outputs

    def _get_stage_model(self, stage: str) -> GPT4All:
        """æŒ‰é˜¶æ®µè¿”å›å¯¹åº”æ¨¡å‹ï¼›è‹¥æœªé…ç½®åˆ†é˜¶æ®µï¼Œåˆ™å›é€€åˆ°å•æ¨¡å‹ã€‚"""
        if self.model_single is not None:
            return self.model_single
        name = None
        if stage == 'filter':
            name = self.model_filter_name
        elif stage == 'abstract':
            name = self.model_abstract_name
        elif stage == 'summarize':
            name = self.model_summarize_name
        elif stage == 'overall':
            name = self.model_overall_name
        elif stage == 'impact':
            name = self.model_impact_name
        if not name:
            # å…œåº•ï¼šä½¿ç”¨æ‘˜è¦æ¨¡å‹æˆ–ä»»ä¸€å¯ç”¨
            name = self.model_abstract_name or self.model_summarize_name or self.model_overall_name or self.model_filter_name
        # ç»Ÿä¸€ï¼šå°è¯•å°†ä¸Šä¸‹æ–‡çª—å£æå‡åˆ° 4096ï¼Œé¿å…é»˜è®¤ 2048 å¯¼è‡´çš„ Context Full æŠ¥é”™
        # è¿™å¯¹æœªå¾®è°ƒçš„ Llama2/3 æ¨¡å‹é€šå¸¸æ˜¯å®‰å…¨çš„ä¸”å¿…è¦çš„
        kwargs = dict(model_path=self.model_path, allow_download=False)
        kwargs['n_ctx'] = 4096
        return GPT4All(name, **kwargs)